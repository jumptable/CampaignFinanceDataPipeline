{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install dj_database_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install dedupe-variable-address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install dedupe-variable-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import optparse\n",
    "import locale\n",
    "import itertools\n",
    "import io\n",
    "import csv\n",
    "\n",
    "import dj_database_url\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "\n",
    "import dedupe\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2.extensions import register_adapter, AsIs\n",
    "register_adapter(numpy.int32, AsIs)\n",
    "register_adapter(numpy.int64, AsIs)\n",
    "register_adapter(numpy.float32, AsIs)\n",
    "register_adapter(numpy.float64, AsIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Readable(object):\n",
    "\n",
    "    def __init__(self, iterator):\n",
    "\n",
    "        self.output = io.StringIO()\n",
    "        self.writer = csv.writer(self.output)\n",
    "        self.iterator = iterator\n",
    "\n",
    "    def read(self, size):\n",
    "\n",
    "        self.writer.writerows(itertools.islice(self.iterator, size))\n",
    "\n",
    "        chunk = self.output.getvalue()\n",
    "        self.output.seek(0)\n",
    "        self.output.truncate(0)\n",
    "\n",
    "        return chunk\n",
    "\n",
    "\n",
    "def record_pairs(result_set):\n",
    "\n",
    "    for i, row in enumerate(result_set):\n",
    "        a_record_id, a_record, b_record_id, b_record = row\n",
    "        record_a = (a_record_id, a_record)\n",
    "        record_b = (b_record_id, b_record)\n",
    "\n",
    "        yield record_a, record_b\n",
    "\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "\n",
    "\n",
    "def cluster_ids(clustered_dupes):\n",
    "\n",
    "    for cluster, scores in clustered_dupes:\n",
    "        cluster_id = cluster[0]\n",
    "        for donor_id, score in zip(cluster, scores):\n",
    "            yield donor_id, cluster_id, score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    # ## Logging\n",
    "\n",
    "    # Dedupe uses Python logging to show or suppress verbose output. Added\n",
    "    # for convenience.  To enable verbose output, run `python\n",
    "    # pgsql_big_dedupe_example.py -v`\n",
    "    optp = optparse.OptionParser()\n",
    "    optp.add_option('-v', '--verbose', dest='verbose', action='count',\n",
    "                    help='Increase verbosity (specify multiple times for more)'\n",
    "                    )\n",
    "    (opts, args) = optp.parse_args()\n",
    "    log_level = logging.WARNING\n",
    "    if opts.verbose:\n",
    "        if opts.verbose == 1:\n",
    "            log_level = logging.INFO\n",
    "        elif opts.verbose >= 2:\n",
    "            log_level = logging.DEBUG\n",
    "    logging.getLogger().setLevel(log_level)\n",
    "\n",
    "    # ## Setup\n",
    "    settings_file = 'pgsql_big_dedupe_example_settings'\n",
    "    training_file = 'pgsql_big_dedupe_example_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Setup\n",
    "    settings_file = 'pgsql_big_dedupe_settings'\n",
    "    training_file = 'pgsql_big_dedupe_training.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    start_time = time.time()\n",
    "    \n",
    "    read_con = psycopg2.connect(database=\"campaign-finance\",\n",
    "                        user=\"data\",\n",
    "                        password=\"data\",\n",
    "                        host=\"postgresdb\",\n",
    "                        port=\"5432\",\n",
    "                        cursor_factory=psycopg2.extras.RealDictCursor)\n",
    "\n",
    "    write_con = psycopg2.connect(database=\"campaign-finance\",\n",
    "                        user=\"data\",\n",
    "                        password=\"data\",\n",
    "                        host=\"postgresdb\",\n",
    "                        port=\"5432\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    DONOR_SELECT = \"SELECT account_id, city, name, zip, state, address, occupation, employer \" \\\n",
    "                   \"from processed_accounts\"\n",
    "\n",
    "    # ## Training\n",
    "\n",
    "    if os.path.exists(settings_file):\n",
    "        print('reading from ', settings_file)\n",
    "        with open(settings_file, 'rb') as sf:\n",
    "            deduper = dedupe.StaticDedupe(sf, num_cores=4)\n",
    "    else:\n",
    "\n",
    "        # Define the fields dedupe will pay attention to\n",
    "        #\n",
    "        # The address, city, and zip fields are often missing, so we'll\n",
    "        # tell dedupe that, and we'll learn a model that take that into\n",
    "        # account\n",
    "        fields = [{'field': 'name', 'type': 'String'},\n",
    "                  {'field': 'address', 'type': 'String', 'has missing': True},\n",
    "                  {'field': 'city', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'state', 'type': 'ShortString', 'has missing': True},\n",
    "                  {'field': 'zip', 'type': 'ShortString', 'has missing': True}#\n",
    "                  ]\n",
    "\n",
    "        # Create a new deduper object and pass our data model to it.\n",
    "        deduper = dedupe.Dedupe(fields, num_cores=8)\n",
    "\n",
    "        # Named cursor runs server side with psycopg2\n",
    "        with read_con.cursor('donor_select') as cur:\n",
    "            cur.execute(DONOR_SELECT)\n",
    "            temp_d = {i: row for i, row in enumerate(cur)}\n",
    "\n",
    "        # If we have training data saved from a previous run of dedupe,\n",
    "        # look for it an load it in.\n",
    "        #\n",
    "        # __Note:__ if you want to train from\n",
    "        # scratch, delete the training_file\n",
    "        if os.path.exists(training_file):\n",
    "            print('reading labeled examples from ', training_file)\n",
    "            with open(training_file) as tf:\n",
    "                deduper.prepare_training(temp_d, tf)\n",
    "        else:\n",
    "            deduper.prepare_training(temp_d)\n",
    "\n",
    "        del temp_d\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # ## Active learning\n",
    "\n",
    "        print('starting active labeling...')\n",
    "        # Starts the training loop. Dedupe will find the next pair of records\n",
    "        # it is least certain about and ask you to label them as duplicates\n",
    "        # or not.\n",
    "\n",
    "        # use 'y', 'n' and 'u' keys to flag duplicates\n",
    "        # press 'f' when you are finished\n",
    "        dedupe.console_label(deduper)\n",
    "        # When finished, save our labeled, training pairs to disk\n",
    "        with open(training_file, 'w') as tf:\n",
    "            deduper.write_training(tf)\n",
    "\n",
    "        # Notice our argument here\n",
    "        #\n",
    "        # `recall` is the proportion of true dupes pairs that the learned\n",
    "        # rules must cover. You may want to reduce this if your are making\n",
    "        # too many blocks and too many comparisons.\n",
    "        deduper.train(recall=0.90)\n",
    "\n",
    "        with open(settings_file, 'wb') as sf:\n",
    "            deduper.write_settings(sf)\n",
    "\n",
    "        # We can now remove some of the memory hogging objects we used\n",
    "        # for training\n",
    "        deduper.cleanup_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    # ## Blocking\n",
    "    print('blocking...')\n",
    "\n",
    "    # To run blocking on such a large set of data, we create a separate table\n",
    "    # that contains blocking keys and record ids\n",
    "    print('creating blocking_map database')\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS blocking_map\")\n",
    "            cur.execute(\"CREATE TABLE blocking_map \"\n",
    "                        \"(block_key text, canon_account_id INTEGER)\")\n",
    "\n",
    "    # If dedupe learned a Index Predicate, we have to take a pass\n",
    "    # through the data and create indices.\n",
    "    print('creating inverted index')\n",
    "\n",
    "    for field in deduper.fingerprinter.index_fields:\n",
    "        with read_con.cursor('field_values') as cur:\n",
    "            cur.execute(\"SELECT DISTINCT %s FROM processed_accounts\" % field)\n",
    "            field_data = (row[field] for row in cur)\n",
    "            deduper.fingerprinter.index(field_data, field)\n",
    "\n",
    "    # Now we are ready to write our blocking map table by creating a\n",
    "    # generator that yields unique `(block_key, donor_id)` tuples.\n",
    "    print('writing blocking map')\n",
    "\n",
    "    with read_con.cursor('donor_select') as read_cur:\n",
    "        read_cur.execute(DONOR_SELECT)\n",
    "\n",
    "        full_data = ((row['account_id'], row) for row in read_cur)\n",
    "        b_data = deduper.fingerprinter(full_data)\n",
    "\n",
    "        with write_con:\n",
    "            with write_con.cursor() as write_cur:\n",
    "                write_cur.copy_expert('COPY blocking_map FROM STDIN WITH CSV',\n",
    "                                      Readable(b_data),\n",
    "                                        size=10000)\n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # free up memory by removing indices\n",
    "    deduper.fingerprinter.reset_indices()\n",
    "\n",
    "    logging.info(\"indexing block_key\")\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"CREATE UNIQUE INDEX ON blocking_map \"\n",
    "                        \"(block_key text_pattern_ops, canon_account_id)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_con.commit()\n",
    "write_con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # ## Clustering\n",
    "\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"DROP TABLE IF EXISTS entity_map\")\n",
    "\n",
    "            print('creating entity_map database')\n",
    "            cur.execute(\"CREATE TABLE entity_map \"\n",
    "                        \"(original_id INTEGER, canon_id INTEGER, \"\n",
    "                        \" cluster_score FLOAT, PRIMARY KEY(original_id))\")\n",
    "\n",
    "    with read_con.cursor('pairs', cursor_factory=psycopg2.extensions.cursor) as read_cur:\n",
    "        read_cur.execute(\"\"\"\n",
    "               select a.account_id,\n",
    "                      row_to_json((select d from (select a.city,\n",
    "                                                         a.name,\n",
    "                                                         a.zip,\n",
    "                                                         a.state,\n",
    "                                                         a.address,\n",
    "                                                         a.occupation,\n",
    "                                                         a.employer) d)),\n",
    "                      b.account_id,\n",
    "                      row_to_json((select d from (select b.city,\n",
    "                                                         b.name,\n",
    "                                                         b.zip,\n",
    "                                                         b.state,\n",
    "                                                         b.address,\n",
    "                                                         b.occupation,\n",
    "                                                         b.employer) d))\n",
    "               from (select DISTINCT l.canon_account_id as east, r.canon_account_id as west\n",
    "                     from blocking_map as l\n",
    "                     INNER JOIN blocking_map as r\n",
    "                     using (block_key)\n",
    "                     where l.canon_account_id < r.canon_account_id) ids\n",
    "               INNER JOIN processed_accounts a on ids.east=a.account_id\n",
    "               INNER JOIN processed_accounts b on ids.west=b.account_id\"\"\")\n",
    "\n",
    "        print('clustering...')\n",
    "        clustered_dupes = deduper.cluster(deduper.score(record_pairs(read_cur)),\n",
    "                                          threshold=0.5)\n",
    "\n",
    "        # ## Writing out results\n",
    "\n",
    "        # We now have a sequence of tuples of donor ids that dedupe believes\n",
    "        # all refer to the same entity. We write this out onto an entity map\n",
    "        # table\n",
    "\n",
    "        print('writing results')\n",
    "        with write_con:\n",
    "            with write_con.cursor() as write_cur:\n",
    "                write_cur.copy_expert('COPY entity_map FROM STDIN WITH CSV',\n",
    "                                      Readable(cluster_ids(clustered_dupes)),\n",
    "                                      size=10000)\n",
    "\n",
    "    with write_con:\n",
    "        with write_con.cursor() as cur:\n",
    "            cur.execute(\"CREATE INDEX head_index ON entity_map (canon_id)\")\n",
    "\n",
    "    # Print out the number of duplicates found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    read_con.close()\n",
    "    write_con.close()\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " locale.setlocale(locale.LC_ALL, '')  # for pretty printing numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    with read_con.cursor() as cur:\n",
    "        cur.execute(\"DROP TABLE e_map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " with read_con.cursor() as cur:\n",
    "   \n",
    "        cur.execute(\n",
    "            \"SELECT CONCAT_WS(' ', donors.name) as name, \"\n",
    "            \"SUM(CAST(contributions.amount AS FLOAT)) AS totals \"\n",
    "            \"FROM donors INNER JOIN contributions \"\n",
    "            \"USING (donor_id) \"\n",
    "            \"GROUP BY (donor_id) \"\n",
    "            \"ORDER BY totals DESC \"\n",
    "            \"LIMIT 10\"\n",
    "        )\n",
    "\n",
    "        print(\"Top Donors (raw)\")\n",
    "        for row in cur:\n",
    "            row['totals'] = row['totals']\n",
    "            print('%(totals)20s: %(name)s' % row)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "    with read_con.cursor() as cur:\n",
    "               \n",
    "        cur.execute(\"CREATE TEMPORARY TABLE e_map \"\n",
    "                    \"AS SELECT COALESCE(canon_id, donor_id) AS canon_id, donor_id \"\n",
    "                    \"FROM entity_map \"\n",
    "                    \"RIGHT JOIN donors USING(donor_id)\")\n",
    "        \n",
    "        cur.execute(\n",
    "            \"SELECT donors.name AS name, \"\n",
    "            \"donation_totals.totals AS totals \"\n",
    "            \"FROM donors INNER JOIN \"\n",
    "            \"(SELECT contributions.canon_id, SUM(CAST(amount AS FLOAT)) AS totals \"\n",
    "            \" FROM contributions INNER JOIN e_map \"\n",
    "            \" USING (donor_id) \"\n",
    "            \" GROUP BY (contributions.canon_id) \"\n",
    "            \" ORDER BY totals \"\n",
    "            \" DESC LIMIT 10) \"\n",
    "            \"AS donation_totals ON donors.donor_id=donation_totals.canon_id \"\n",
    "            \"WHERE donors.donor_id = donation_totals.canon_id\"\n",
    "        )\n",
    "\n",
    "        print(\"Top Donors (deduped)\")\n",
    "        for row in cur:\n",
    "            row['totals'] = row['totals']\n",
    "            print('%(totals)20s: %(name)s' % row)\n",
    "            \n",
    "        cur.execute(\"SELECT * FROM e_map\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    read_con.close()\n",
    "    write_con.close()\n",
    "\n",
    "    print('ran in', time.time() - start_time, 'seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
